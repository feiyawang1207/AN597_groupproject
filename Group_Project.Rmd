---
title: "Group_Project"
author: "Feiya Wang"
date: "11/11/2019"
output: html_document
---
***
## Group Project--Species Distribution Model

***
It is a Group projetc worked by Majo, Frank and Feiya

## Introduction: short paragraph and an image
Species distribution modeling (SDM) is also known under other names including climate envelope-modeling, habitat modeling, and (environmental or ecological) niche-modeling. 

The aim of SDM is to estimate the similarity of the conditions at any site to the conditions at the locations of known occurrence (and perhaps of non-occurrence) of a phenomenon. A common application of this method is to predict species ranges with climate data as predictors. 

SDM steps: 
(1) locations of occurrence of a species (or other phenomenon) are compiled
(2) values of environmental predictor variables (such as climate) at these locations are extracted from spatial databases
(3) the environmental values are used to fit a model to estimate similarity to the sites of occurrence, or another measure such as abundance of the species 
(4) The model is used to predict the variable of interest across the region of interest (and perhaps for a future or past climate).
Before start it would be useful if you get familiar with spatial data handling as implemented in packages “raster” and “sp”


>Packages to be used: install.packages(c(’raster’, ’rgdal’, ’dismo’, ’rJava’)) 

# Step 1: Data preparation - due Monday 18th
You need to collect a sufficient number of occurrence records (and absence or abundance, if possible) of the species of interest. You also need to have accurate an relevant environmental data (predictor variables) at sufficient high spatial resolution.

## Species occurrence data
Importing occurrence data into R is easy. But collecting, georeferencing, and cross-checking coordinate data is tedious
>1.Upload files

If You have a files, than you can upload it like we did in previous class, using curl package. 

For example:

```{r}
library (curl)
#install package curl before this step to load a file from server.
```


```{r}
f <- curl("https://github.com/feiyawang1207/AN597_groupproject/blob/master/S_angustifolium.xlsx")
#load the file in varibale f
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
# read file in d as a dataframe.

head(d)
```

> 1.Importing occurrence data

If you do not have any species distribution data you can get started by downloading data from the Global Biodiversity Inventory Facility (GBIF).
Occurrence records: 1.354.603.236

Importing data from a database, we use a function called gbif() to download the files from this database. 

We are going to download the occurrence data from a potato species. Solanum acaule is widespread and common in upland habitats from northern Peru (Dept. Cajamarca), south through Bolivia to northern Argentina (Prov. San Juan), and with one record in northern Chile (Antofagasta Region), on dry rocky hillsides, high puna, among herbs, spiny shrubs and low woods, along streamsides, dry river beds and alluvial cones, (2000) 2400-4700 m in elevation.  IMAGE
```{r}
library(dismo)
#library(DT)

acaule <- gbif("solanum", "acaule*", geo=FALSE)
#load the saved S. acaule data from data base

datatable(head(acaule))
#
```

If you want to check how many rows and colums the data frame you have, we use dim() to show the dimension of the dataset.
```{r}
colrow<-dim(acaule)
# use dim() to get dimension of our dataset

colrow[1]
#show row numbers

y<-colrow[2]
#show column number
```

In order to drawing a map, we need to find the location by Longitude and latitude from dataset. 

Subetting the data that have longitude and latitude. Many occurence records may not have geographic coordinates, so you will need to remove those NA values from your dataframe. We use is.na() to test the data that have both longitude and latitude data and make them into a data frame.
```{r}
acgeo <- subset(acaule, !is.na(lon) & !is.na(lat))
#use subset() to seperate those data

#datatable(head(acgeo))
#showing the example data see that have longtitude and latitude data

dim(acgeo)
#showing the rows and columns for data frame
```
show some values from your dataframe
# ???
I don't understand this function.
```{r}
acgeo[1:4, c(1:5,7:10)] #removing locality
```

> 2.Making a map of the ocurrence localities of Solanum acaule



In order to do that, we need package maptools. 

First, we need to draw a empty map. a variable called wrld_simpl will show a empty world-wide map. The wrld_simpl dataset contains rough country outlines.
```{r}
library(maptools)

data(wrld_simpl)
x<-plot(wrld_simpl, xlim=c(-80,70), ylim=c(-80,30), axes=TRUE)
# empty map
```


Then we need to add our data points. It is important to make such maps to assure that the points are, at least roughly, in the right location
```{r}
library(maptools)

data(wrld_simpl)
plot(wrld_simpl, xlim=c(-80,70), ylim=c(-80,30), axes=TRUE, col="light yellow")
# restore the box around the map
box()
# add the points
points(acgeo$lon, acgeo$lat, col='orange', pch=20, cex=0.75)
# plot points again to add a border, for better visibility
points(acgeo$lon, acgeo$lat, col='red', cex=0.75)
```

```{r we should remove this}
 r <- raster(system.file("external/test.grd", package="raster"))

```

>3. Data cleaning-majo

Data ‘cleaning’ is particularly important for data sourced from species distribution data warehouses such as GBIF.
Solanum acaule is a species that occurs in the higher parts of the Andes mountains of southern Peru, Bolivia and northern Argentina. 

IMAGE

Do you see any errors on the map?

There are a few records that map in weird locations. Sometimes is just because a missing minus signs.
There are two records (rows 303 and 885) that map to the same spot in Antarctica (-76.3, -76.3). The locality description says that is should be in Huarochiri, near Lima, Peru. So the longitude is probably correct, and erroneously copied to the latitude
```{r}
acaule[c(303,885),1:10]
```

The point in Brazil (record acaule[98,]) should be in soutern Bolivia, so this is probably due to a typo in the longitude.
There are also three records that have plausible latitudes, but longitudes that are clearly wrong, as they are in the Atlantic Ocean, south of West Africa. It looks like they have a longitude that is zero. In many data-bases you will find values that are ‘zero’ where ‘no data’ was intended. The gbif function (when using the default arguments) sets coordinates that are (0, 0) to NA, but not if one of the coordinates is zero. Let’s see if we find them by searching for records with longitudes of zero.
```{r}
lonzero = subset(acgeo, lon==0)
lonzero[, 1:13]
```
The records are from Bolivia, Peru and Argentina, confirming that coordinates are in error. 

>Duplicate records: The next function helps to remove duplicates.

```{r}
#which records are duplicates (only for the first 10 columns)?
dups <- duplicated(lonzero[, 1:10])
#remove duplicates
lonzero  <-  lonzero[dups, ]
lonzero[,1:13]
```
>Another approach might be to detect duplicates for the same species and some coordinates in the data, even if the records were from collections by different people or in different years (in our case, using species is redundant as we have data for only one species).

```{r}
# differentiating by (sub) species
# dups2 <- duplicated(acgeo[, c('species', 'lon', 'lat')])
# ignoring (sub) species and other naming variation
dups2 <- duplicated(acgeo[, c('lon', 'lat')])
# number of duplicates
sum(dups2)
## [1] 483
# keep the records that are _not_ duplicated
acg <- acgeo[!dups2, ]
```

>Let’s repatriate the records near Pakistan to Argentina, and remove the records in Brazil, Antarctica, and with longitude=0

```{r}
i <- acg$lon > 0 & acg$lat > 0
acg$lon[i] <- -1 * acg$lon[i]
acg$lat[i] <- -1 * acg$lat[i]
acg <- acg[acg$lon < -50 & acg$lat > -50, ]
```

>Cross-checking: 

```{r}
library(sp)
coordinates(acg) <- ~lon+lat
crs(acg) <- crs(wrld_simpl)
class(acg)
## [1] "SpatialPointsDataFrame"
## attr(,"package")
## [1] "sp"
```

```{r}
class(wrld_simpl)
## [1] "SpatialPolygonsDataFrame"
## attr(,"package")
## [1] "sp"
ovr <- over(acg, wrld_simpl)
```

```{r}
head(ovr)
cntr <- ovr$NAME
```

```{r}
i <- which(is.na(cntr))
i
## integer(0)
j <- which(cntr != acg$country)
# for the mismatches, bind the country names of the polygons and points
cbind(cntr, acg$country)[j,]
##      cntr
## [1,] "27"  "Argentina"
## [2,] "172" "Bolivia"
## [3,] "172" "Bolivia"
## [4,] "172" "Bolivia"
```

```{r}
plot(acg)
plot(wrld_simpl, add=T, border='blue', lwd=2)
points(acg[j, ], col='red', pch=20, cex=2)
```

>Absence and background points: 

Some of the early species distribution model algorithms, such as Bioclim and Domain only use ’presence’ data in the modeling process. Other methods also use ’absence’ data or ’background’ data.

Therefore, if you want to have a non-NA-value data frame for your method, Here is the thing you could do.

First, there is a common methods to delete all NA values from a data frame. We can use na.omit() function to delete the rows that contains the NA data.
 For example, you only need longtitude and latitude values to draw your map, then, you can take those two columns into a data frame. 

```{r}
long<-acaule$lon
#extract column of longtitude
lati<-acaule$lat
#extract column of latitude
d<-cbind(long,lati)
#combine into a data frame
colnames(d)<-c("longitude","latitude")
#name data frame

dim(d)[1]
#rows number of uncleaning data frame

d[27,]
#a NA rows in this data frame
```

Then, you can use na.omit() to delete all NA value from this data frame. 
```{r}
clean_d <-na.omit(d)
#save teh cleaing data frame

dim(clean_d)[1]
#rows number of cleaning data frame

clean_d[27,]
#NA have been removed from the previous row.

```

The dismo package also have some function to delete NA data from the data frame.
For example, raster()
 


# Environmental data Due 25th

>Raster data

A raster consists of a matrix of cells (or pixels) organized into rows and columns (or a grid) where each contains a value representing information. Raster format represents real-world phenomena, data might be discrete or continuous. 
Discrete data: also are called thematic, categorical or discontinuous data. A discrete object has known and definable boundaries. 
In species distribution modeling, predictor variables are typically organized as raster (grid) type files. Each predictor should be a ‘raster’ representing a variable of interest. Variables can include climatic, soil, terrain, vegetation, land use, and other variables.
These data are typically stored in files in some kind of GIS format. Almost all relevant formats can be used (including ESRI grid, geoTiff, netCDF, IDRISI).
For any particular study the layers should all have the same spatial extent, resolution, origin, and projection. 
The set of predictor variables (rasters) can be used to make a RasterStack, which is a collection of `RasterLayer’ objects 

A RasterLayer object represents single-layer (variable) raster data. A RasterLayer object always stores a number of fundamental parameters that describe it. These include the number of columns and rows, the spatial extent, and the Coordinate Reference System. In addition, a RasterLayer can store information about the file in which the raster cell values are stored (if there is such a file). A RasterLayer can also hold the raster cell values in memory.

Using the dismo package we are going to create a rasterStack from the list of files are intalled in the package. 
Fist, we need to get the folder with our files.....

```{r}
path <- file.path(system.file(package="dismo"), 'ex')
```

Now get the names of all the files with extension “.grd” in this folder. The $ sign indicates that the files must end with
the characters ‘grd’. By using full.names=TRUE, the full path names are retunred.

```{r}
library(dismo)
files <- list.files(path, pattern='grd$', full.names=TRUE )
files
```

Now create a RasterStack of predictor variables.

```{r}
predictors <- stack(files)
predictors
```

```{r}
names(predictors)
```


```{r}
plot(predictors)
```

We can also make a plot of a single layer in a RasterStack, and plot some additional data on top of it. First get the world boundaries and the bradypus data:
```{r}
library(maptools)
data(wrld_simpl)
file <- paste(system.file(package="dismo"), "/ex/bradypus.csv", sep="")
bradypus <- read.table(file,  header=TRUE,  sep=',')
# we do not need the first column
bradypus  <- bradypus[,-1]
```

Plotting bioclimatic variables from WorldClim database

```{r}
# first layer of the RasterStack
plot(predictors, 1)
# note the "add=TRUE" argument with plot
plot(wrld_simpl, add=TRUE)
# with the points function, "add" is implicit
points(bradypus, col='blue')
```

>Extracting values from rasters



# Model fitting, prediction, and evaluation
>Model fitting

>Model prediction

>Model evaluation

Most model types have different measures that can help to assess how good the model
fits the data. Most statistics or machine learning texts will provide
some details. For instance, for a GLM one can look at how much deviance is explained, whether there are patterns in the residuals, whether there are points with high leverage and so on. However, since many models are to be used for prediction, much evaluation is focused on how well the model predicts to points not used in model training.
What do we consider to evaluate a model?

Does the model seem sensible, ecologically?
Do the fitted functions (the shapes of the modeled relationships) make sense?
Do the predictions seem reasonable? (map them, and think about them)?
Are there any spatial patterns in model residuals? 


Different measures can be used to evaluate the quality of a prediction, perhaps depending on the goal of the study. Many measures for evaluating models based on presence-absence or presence-only data are ‘threshold dependent’. That means that a threshold must be set first (e.g., 0.5, though 0.5 is rarely a sensible choice – e.g. see Lui et al. 2005). Predicted values above that threshold indicate a prediction of ‘presence’, and values below the threshold indicate ‘absence’. Some measures emphasize the weight of false absences; others give more weight to false presences.
Much used statistics that are threshold independent are the correlation coefficient and the Area Under the Receiver Operator Curve (AUROC, generally further abbreviated to AUC). AUC is a measure of rank-correlation. In unbiased data, a high AUC indicates that sites with high predicted suitability values tend to be areas of known presence and locations with lower model prediction values tend to be areas where the species is not known to be present (absent or a random point). An AUC score of 0.5 means that the model is as good as a random guess. 
for a discussion on the use of AUC in the context of presence-only rather than presence/absence data.

Here we illustrate the computation of the correlation coefficient and AUC with two random variables. p (presence) has higher values, and represents the predicted value for 50 known cases (locations) where the species is present, and a (absence) has lower values, and represents the predicted value for 50 known cases (locations) where the species is absent.
```{r}
p <- rnorm(50, mean=0.7, sd=0.3)
a <- rnorm(50, mean=0.4, sd=0.4)
par(mfrow=c(1, 2))
plot(sort(p), col='red', pch=21)
points(sort(a), col='blue', pch=24)
legend(1, 0.95 * max(a,p), c('presence', 'absence'),
          pch=c(21,24), col=c('red', 'blue'))
comb <- c(p,a)
group <- c(rep('presence', length(p)), rep('absence', length(a)))
boxplot(comb~group, col=c('blue', 'red'))
```

We created two variables with random normally distributed values, but with different mean and standard deviation.
The two variables clearly have different distributions, and the values for ‘presence’ tend to be higher than for ‘absence’. 
Here is how you can compute the correlation coefficient and the AUC:
```{r}
group = c(rep(1, length(p)), rep(0, length(a)))
cor.test(comb, group)$estimate
```


```{r}
mv <- wilcox.test(p,a)
auc <- as.numeric(mv$statistic) / (length(p) * length(a))
auc
```

Below we show how you can compute these, and other statistics more conveniently, with the evaluate function in the dismo package. See ?evaluate for info on additional evaluation measures that are available. ROC/AUC can also be computed with the ROCR package.
```{r}
e <- evaluate(p=p, a=a)
class(e)

par(mfrow=c(1, 2))
density(e)
boxplot(e, col=c('blue', 'red'))
```

Back to some real data, presence-only in this case. We’ll divide the data in two random sets, one for training a Bioclim
model, and one for evaluating the model.

```{r}
samp <- sample(nrow(sdmdata), round(0.75 * nrow(sdmdata)))
traindata <- sdmdata[samp,]
traindata <- traindata[traindata[,1] == 1, 2:9]
testdata <- sdmdata[-samp,]
bc <- bioclim(traindata)
e <- evaluate(testdata[testdata==1,], testdata[testdata==0,], bc)
e
```


```{r}
plot(e, 'ROC')
```

Let’s first create presence and background data.

```{r}
pres <- sdmdata[sdmdata[,1] == 1, 2:9]
back <- sdmdata[sdmdata[,1] == 0, 2:9]
```

The background data will only be used for model testing and does not need to be partitioned. We now partition the data into 5 groups

```{r}
k <- 5
group <- kfold(pres, k)
group[1:10]

unique(group)

```
Now we can fit and test our model five times. In each run, the records corresponding to one of the five groups is only used to evaluate the model, while the other four groups are only used to fit the model. The results are stored in a list called ‘e’.

```{r}
e <- list()
for (i in 1:k) {
    train <- pres[group != i,]
    test <- pres[group == i,]
    bc <- bioclim(train)
    e[[i]] <- evaluate(p=test, a=back, bc)
}
```

We can extract several things from the objects in ‘e’, but let’s restrict ourselves to the AUC values and the “maximum of the sum of the sensitivity (true positive rate) and specificity (true negative rate)” threshold “spec_sens” (this is sometimes uses as a threshold for setting cells to presence or absence).

```{r}
auc <- sapply(e, function(x){x@auc})
auc
```


```{r}
mean(auc)

sapply( e, function(x){ threshold(x)['spec_sens'] } )
```
The use of AUC in evaluating SDMs has been criticized (Lobo et al. 2008, Jiménez-Valverde 2011). A particularly sticky problem is that the values of AUC vary with the spatial extent used to select background points. Generally, the larger that extent, the higher the AUC value. Therefore, AUC values are generally biased and cannot be directly compared. Hijmans (2012) suggests that one could remove “spatial sorting bias” (the difference between the distance from testing-presence to training-presence and the distance from testing-absence to training-presence points) through “point-wise distance sampling”.


```{r}
file <- file.path(system.file(package="dismo"), "ex/bradypus.csv")
bradypus <- read.table(file,  header=TRUE,  sep=',')
bradypus <- bradypus[,-1]
presvals <- extract(predictors, bradypus)
set.seed(0)
backgr <- randomPoints(predictors, 500)
nr <- nrow(bradypus)
s <- sample(nr, 0.25 * nr)
pres_train <- bradypus[-s, ]
pres_test <- bradypus[s, ]
nr <- nrow(backgr)
set.seed(9)
s <- sample(nr, 0.25 * nr)
back_train <- backgr[-s, ]
back_test <- backgr[s, ]
```
 
 
 
```{r}
sb <- ssb(pres_test, back_test, pres_train)
sb[,1] / sb[,2]
```

sb[,1] / sb[,2] is an indicator of spatial sorting bias (SSB). If there is no SSB this value should be 1, in these
data it is close to zero, indicating that SSB is very strong. Let’s create a subsample in which SSB is removed.

```{r}
i <- pwdSample(pres_test, back_test, pres_train, n=1, tr=0.1)
pres_test_pwd <- pres_test[!is.na(i[,1]), ]
back_test_pwd <- back_test[na.omit(as.vector(i)), ]
sb2 <- ssb(pres_test_pwd, back_test_pwd, pres_train)
sb2[1]/ sb2[2]
```

Spatial sorting bias is much reduced now; notice how the AUC dropped!

```{r}
bc <- bioclim(predictors, pres_train)
evaluate(bc, p=pres_test, a=back_test, x=predictors)
```



```{r}
evaluate(bc, p=pres_test_pwd, a=back_test_pwd, x=predictors)
```


>Types of models

# Generalized Linear Models: example









```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

