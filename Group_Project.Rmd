---
title: "Group_Project"
author: "Feiya Wang"
date: "11/11/2019"
output: html_document
---
***
## **Group Project--Species Distribution Model**

***
It is a Group projetc worked by Majo, Frank and Feiya

# Introduction: short paragraph and an image

Species distribution modeling (SDM) is also known under other names including climate envelope-modeling, habitat modeling, and (environmental or ecological) niche-modeling. 

The aim of SDM is to estimate the similarity of the conditions at any site to the conditions at the locations of known occurrence (and perhaps of non-occurrence) of a phenomenon. A common application of this method is to predict species ranges with climate data as predictors. 

SDM steps: 

(1) locations of occurrence of a species (or other phenomenon) are compiled

(2) values of environmental predictor variables (such as climate) at these locations are extracted from spatial databases

(3) the environmental values are used to fit a model to estimate similarity to the sites of occurrence, or another measure such as abundance of the species 

(4) The model is used to predict the variable of interest across the region of interest (and perhaps for a future or past climate).
Before start it would be useful if you get familiar with spatial data handling as implemented in packages “raster” and “sp”


Packages to be used: *raster*, *rgdal*, *dismo*, *rJava* 

# Step 1: Data preparation

You need to collect a sufficient number of occurrence records (and absence or abundance, if possible) of the species of interest. You also need to have accurate an relevant environmental data (predictor variables) at sufficient high spatial resolution.

# Step 2: Import Species occurrence data

Importing occurrence data into R is easy. But collecting, georeferencing, and cross-checking coordinate data is tedious

>1.Upload files

If You have a files, than you can upload it like we did in previous class, using curl package. 

For example:

```{r}
library (curl)
#install package curl before this step to load a file from server.
library(knitr)
# to make nice data table. 

f <- curl("https://raw.githubusercontent.com/feiyawang1207/AN597_groupproject/master/S_angustifolium.csv")
#load the file in varibale f
d <- read.csv(f, header = TRUE, sep = "\t", stringsAsFactors = FALSE)
# read file in d as a dataframe.
kable(head(d))
```

>2.Importing occurrence data

If you do not have any species distribution data you can get started by downloading data from the Global Biodiversity Inventory Facility (GBIF).
Occurrence records: 1.354.603.236

Importing data from a database, we use a function called gbif() to download the files from this database. 

We are going to download the occurrence data from a potato species. Solanum acaule is widespread and common in upland habitats from northern Peru (Dept. Cajamarca), south through Bolivia to northern Argentina (Prov. San Juan), and with one record in northern Chile (Antofagasta Region), on dry rocky hillsides, high puna, among herbs, spiny shrubs and low woods, along streamsides, dry river beds and alluvial cones, (2000) 2400-4700 m in elevation.  IMAGE
```{r}
library(dismo)
library(knitr)

acaule <- gbif("solanum", "acaule*", geo=FALSE)
#load the saved S. acaule data from data base

kable(head(acaule))
```

If you want to check how many rows and colums the data frame you have, we use dim() to show the dimension of the dataset.
```{r}
colrow<-dim(acaule)
# use dim() to get dimension of our dataset

colrow[1]
#show row numbers

y<-colrow[2]
#show column number
```

In order to drawing a map, we need to find the location by Longitude and latitude from dataset. 

Subetting the data that have longitude and latitude. Many occurence records may not have geographic coordinates, so you will need to remove those NA values from your dataframe. We use is.na() to test the data that have both longitude and latitude data and make them into a data frame.
```{r}
acgeo <- subset(acaule, !is.na(lon) & !is.na(lat))
#use subset() to seperate those data

#datatable(head(acgeo))
#showing the example data see that have longtitude and latitude data

dim(acgeo)
#showing the rows and columns for data frame
```

```{r}
acgeo[1:4, c(1:5,7:10)] #removing locality
```


>3. Making a map of the ocurrence localities 

We use solanum acaule as an example

In order to do that, we need package maptools. 

First, we need to draw a empty map. a variable called wrld_simpl will show a empty world-wide map. The wrld_simpl dataset contains rough country outlines.
```{r}
library(maptools)

data(wrld_simpl)
x<-plot(wrld_simpl, xlim=c(-80,70), ylim=c(-80,30), axes=TRUE)
# empty map
```


Then we need to add our data points. It is important to make such maps to assure that the points are, at least roughly, in the right location
```{r}
library(maptools)

data(wrld_simpl)
plot(wrld_simpl, xlim=c(-80,70), ylim=c(-80,30), axes=TRUE, col="light yellow")
# restore the box around the map
box()
# add the points
points(acgeo$lon, acgeo$lat, col='orange', pch=20, cex=0.75)
# plot points again to add a border, for better visibility
points(acgeo$lon, acgeo$lat, col='red', cex=0.75)
```

```{r we should remove this}
 r <- raster(system.file("external/test.grd", package="raster"))
```

>4. Data cleaning

Data ‘cleaning’ is particularly important for data sourced from species distribution data warehouses such as GBIF.
Solanum acaule is a species that occurs in the higher parts of the Andes mountains of southern Peru, Bolivia and northern Argentina. 

IMAGE

Do you see any errors on the map?

There are a few records that map in weird locations. Sometimes is just because a missing minus signs.
There are two records (rows 303 and 885) that map to the same spot in Antarctica (-76.3, -76.3). The locality description says that is should be in Huarochiri, near Lima, Peru. So the longitude is probably correct, and erroneously copied to the latitude
```{r}
acaule[c(303,885),1:10]
```

The point in Brazil (record acaule[98,]) should be in soutern Bolivia, so this is probably due to a typo in the longitude.
There are also three records that have plausible latitudes, but longitudes that are clearly wrong, as they are in the Atlantic Ocean, south of West Africa. It looks like they have a longitude that is zero. In many data-bases you will find values that are ‘zero’ where ‘no data’ was intended. The gbif function (when using the default arguments) sets coordinates that are (0, 0) to NA, but not if one of the coordinates is zero. Let’s see if we find them by searching for records with longitudes of zero.
```{r}
lonzero = subset(acgeo, lon==0)
lonzero[, 1:13]
```
The records are from Bolivia, Peru and Argentina, confirming that coordinates are in error. 


>5.Duplicate records: The next function helps to remove duplicates.

```{r}
#which records are duplicates (only for the first 10 columns)?
dups <- duplicated(lonzero[, 1:10])
#remove duplicates
lonzero  <-  lonzero[dups, ]
lonzero[,1:13]
```

Another approach might be to detect duplicates for the same species and some coordinates in the data, even if the records were from collections by different people or in different years (in our case, using species is redundant as we have data for only one species).

```{r}
# differentiating by (sub) species
# dups2 <- duplicated(acgeo[, c('species', 'lon', 'lat')])
# ignoring (sub) species and other naming variation
dups2 <- duplicated(acgeo[, c('lon', 'lat')])
# number of duplicates
sum(dups2)
## [1] 483
# keep the records that are _not_ duplicated
acg <- acgeo[!dups2, ]
```

Let’s repatriate the records near Pakistan to Argentina, and remove the records in Brazil, Antarctica, and with longitude=0

```{r}
i <- acg$lon > 0 & acg$lat > 0
acg$lon[i] <- -1 * acg$lon[i]
acg$lat[i] <- -1 * acg$lat[i]
acg <- acg[acg$lon < -50 & acg$lat > -50, ]
```

>6. Cross-checking: 

```{r}
library(sp)
coordinates(acg) <- ~lon+lat
crs(acg) <- crs(wrld_simpl)
class(acg)
## [1] "SpatialPointsDataFrame"
## attr(,"package")
## [1] "sp"
```

```{r}
class(wrld_simpl)
## [1] "SpatialPolygonsDataFrame"
## attr(,"package")
## [1] "sp"
ovr <- over(acg, wrld_simpl)
```

```{r}
head(ovr)
cntr <- ovr$NAME
```

```{r}
i <- which(is.na(cntr))
i
## integer(0)
j <- which(cntr != acg$country)
# for the mismatches, bind the country names of the polygons and points
cbind(cntr, acg$country)[j,]

```

```{r}
plot(acg)
plot(wrld_simpl, add=T, border='blue', lwd=2)
points(acg[j, ], col='red', pch=20, cex=2)
```


# Step 3: Deal with absence and background points 

Some of the early species distribution model algorithms, such as Bioclim and Domain only use ’presence’ data in the modeling process. Other methods also use ’absence’ data or ’background’ data.

>1. Absent data

If you want to have a non-NA-value data frame for your method, Here is the thing you could do.

First, there is a common methods to delete all NA values from a data frame. We can use na.omit() function to delete the rows that contains the NA data.
For example, you only need longtitude and latitude values to draw your map, then, you can take those two columns into a data frame. 

```{r}
long<-acaule$lon
#extract column of longtitude
lati<-acaule$lat
#extract column of latitude
d<-cbind(long,lati)
#combine into a data frame
colnames(d)<-c("longitude","latitude")
#name data frame

dim(d)[1]
#rows number of uncleaning data frame

d[27,]
#a NA rows in this data frame
```

Then, you can use na.omit() to delete all NA value from this data frame. 
```{r}
clean_d <-na.omit(d)
#save teh cleaing data frame

dim(clean_d)[1]
#rows number of cleaning data frame

clean_d[27,]
#NA have been removed from the previous row.

```

A simple way to do this is filter function: filter() in dplyr package.
For example:
```{r}
library(dplyr)
new_d<-filter(acaule, lon !="NA" & lat !="NA" )
#new variable to store the data frame.
summary(is.na(new_d$lon))
#use is.na() to check if there is remaining NA in longitude column.
```

>2. Backgroud data

Background data  are not attempting to guess at absence locations, but rather to characterize environments in the study region. In this sense, background is the same, irrespective of where the species has been found.

Background data establishes the environmental domain of the study, whilst presence data should establish under which conditions a species is more likely to be present than on average. A closely related but different concept, that of “pseudo-absences”, is also used for generating the non-presence class for logistic models. In this case, researchers sometimes try to guess where absences might occur – they may sample the whole region except at presence locations, or they might sample at places unlikely to be suitable for the species. We prefer the background concept because it requires fewer assumptions and has some coherent statistical methods for dealing with the “overlap” between presence and background points.


In the example below, we first get the list of filenames with the predictor raster data (discussed in detail in the next section raster). We use a raster as a ‘mask’ in the randomPoints function such that the background points are from the same geographic area, and only for places where there are values (land, in our case).

```{r}
library(dismo)
# get the file names
files <- list.files(path=paste(system.file(package="dismo"), '/ex',
                       sep=''),  pattern='grd',  full.names=TRUE )
# we use the first file to create a RasterLayer
mask <- raster(files[1])
# select 500 random points

set.seed(1963)
# set seed to assure that the examples will always have the same random sample.

bg <- randomPoints(mask, 500 )

plot(!is.na(mask), legend=FALSE)

points(bg, cex=0.5)
```

```{r}
# now we repeat the sampling, but limit the area of sampling using a spatial extent
e <- extent(-80, -53, -39, -22)
bg2 <- randomPoints(mask, 50, ext=e)
#limit point for longitude -39~-22, latitude -80~-53

plot(!is.na(mask), legend=FALSE)
plot(e, add=TRUE, col='red')
points(bg2, cex=0.5)

```


There are several approaches one could use to sample ‘pseudo-absence’ points from more restricted area than ‘background’.
We first read the cleaned and subsetted S. acaule data that we produced in the previous chapter from the csv file that comes with dismo:
```{r}
file <- paste(system.file(package="dismo"), '/ex/acaule.csv', sep='')
ac <- read.csv(file)
#load the cleaned data file

coordinates(ac) <- ~lon+lat
projection(ac) <- CRS('+proj=longlat +datum=WGS84')
#create a Spatial Points Data Frame
```


We first create a ‘circles’ model (see the chapter about geographic models), using an arbitrary radius of 50 km. Circles() is a function from rgeos package.
```{r}
library(rgeos)

x <- circles(ac, d=50000, lonlat=TRUE)
# circles with a radius of 50 km
pol <- polygons(x)

samp1 <- spsample(pol, 250, type='random', iter=25)
# sample randomly from all circles

cells <- cellFromXY(mask, samp1)
length(cells)

cells <- unique(cells)
# get unique cells
length(cells)

xy <- xyFromCell(mask, cells)
plot(pol, axes=TRUE)
points(xy, cex=0.75, pch=20, col='blue')
#make a plot to see results
```

A similar results could be produced via raster function extract()
```{r}
# extract cell numbers for the circles
v <- extract(mask, x@polygons, cellnumbers=T)
# use rbind to combine the elements in list v
v <- do.call(rbind, v)
# get unique cell numbers from which you could sample
v <- unique(v[,1])
head(v)
# to display the results

m <- mask
m[] <- NA
m[v] <- 1
plot(m, ext=extent(x@polygons)+1)
plot(x@polygons, add=T)
# make a plot for results,
```

# step 4: Use R package Raster to analysis data

>Raster data

A raster consists of a matrix of cells (or pixels) organized into rows and columns (or a grid) where each contains a value representing information. Raster format represents real-world phenomena, data might be discrete or continuous. 
Discrete data: also are called thematic, categorical or discontinuous data. A discrete object has known and definable boundaries. 

In species distribution modeling, predictor variables are typically organized as raster (grid) type files. Each predictor should be a ‘raster’ representing a variable of interest. Variables can include climatic, soil, terrain, vegetation, land use, and other variables.

These data are typically stored in files in some kind of GIS format. Almost all relevant formats can be used (including ESRI grid, geoTiff, netCDF, IDRISI).

For any particular study the layers should all have the same spatial extent, resolution, origin, and projection. 
The set of predictor variables (rasters) can be used to make a RasterStack, which is a collection of `RasterLayer’ objects 

A RasterLayer object represents single-layer (variable) raster data. A RasterLayer object always stores a number of fundamental parameters that describe it. These include the number of columns and rows, the spatial extent, and the Coordinate Reference System. In addition, a RasterLayer can store information about the file in which the raster cell values are stored (if there is such a file). A RasterLayer can also hold the raster cell values in memory.

Using the dismo package we are going to create a rasterStack from the list of files are intalled in the package. 
Fist, we need to get the folder with our files.....

```{r}
path <- file.path(system.file(package="dismo"), 'ex')
```

Now get the names of all the files with extension “.grd” in this folder. The $ sign indicates that the files must end with
the characters ‘grd’. By using full.names=TRUE, the full path names are retunred.

```{r}
library(dismo)
files <- list.files(path, pattern='grd$', full.names=TRUE )
files
```

Now create a RasterStack of predictor variables.

```{r}
predictors <- stack(files)
predictors
names(predictors)
plot(predictors)
```

We can also make a plot of a single layer in a RasterStack, and plot some additional data on top of it. First get the world boundaries and the bradypus data:
```{r}
library(maptools)
data(wrld_simpl)
file <- paste(system.file(package="dismo"), "/ex/bradypus.csv", sep="")
bradypus <- read.table(file,  header=TRUE,  sep=',')
# we do not need the first column
bradypus  <- bradypus[,-1]
```

Plotting bioclimatic variables from WorldClim database

```{r}
# first layer of the RasterStack
plot(predictors, 1)
# note the "add=TRUE" argument with plot
plot(wrld_simpl, add=TRUE)
# with the points function, "add" is implicit
points(bradypus, col='blue')
```


>Extracting values from rasters

We now have a set of predictor variables (rasters) and occurrence points. The next step is to extract the values of the predictors at the locations of the points. (This step can be skipped for the modeling methods that are implemented in the dismo package). This is a very straightforward thing to do using the ‘extract’ function from the raster package.
```{r}
presvals <- extract(predictors, bradypus)
# setting random seed to always create the same
# random set of points for this example
set.seed(0)
#Set the seed of R's random number generator, which is useful for creating simulations or random objects that can be reproduced

backgr <- randomPoints(predictors, 500)
#setting for backgroud

absvals <- extract(predictors, backgr)
#extract values

pb <- c(rep(1, nrow(presvals)), rep(0, nrow(absvals)))

sdmdata <- data.frame(cbind(pb, rbind(presvals, absvals)))

sdmdata[,'biome'] = as.factor(sdmdata[,'biome'])

head(sdmdata)
tail(sdmdata)

summary(sdmdata)

```

To visually investigate colinearity in the environmental data (at the presence and background points) you can use a pairs plot.
```{r}
pairs(sdmdata[,2:5], cex=0.1)
```

# Step 5: Picking models

A large number of algorithms has been used in species distribution modeling. They can be classified as ‘profile’, ‘regression’, and ‘machine learning’ methods. Profile methods only consider ‘presence’ data, not absence or background data. Regression and machine learning methods use both presence and absence or background data. The distinction between regression and machine learning methods is not sharp, but it is perhaps still useful as way to classify models. Another distinction that one can make is between presence-only and presence-absence models. Profile methods are always presence-only, other methods can be either, depending if they are used with survey-absence or with pseudo-absence/background data. An entirely different class of models consists of models that only, or primarily, use the geographic location of known occurrences, and do not rely on the values of predictor variables at these locations. We refer to these models as ‘geographic models’.

>Example: Generalized Linear Models

A generalized linear model (GLM) is a generalization of ordinary least squares regression. Models are fit using maximum likelihood and by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value. Depending on how a GLM is specified it can be equivalent to (multiple) linear regression, logistic regression or Poisson regression


In R, GLM is implemented in the ‘glm’ function, and the link function and error distribution are specified with the ‘family’ argument. Examples are:

family = binomial(link = "logit")

family = poisson(link = "log")

Here we fit two basic glm models. The models need to be fit with presence and absence (background) data. With the exception of ‘maxent’, we cannot fit the model with a RasterStack and points. Instead, we need to extract the environmental data values ourselves, and fit the models with these values.

```{r,include=FALSE}
pred_nf <- dropLayer(predictors, 'biome')
group <- kfold(bradypus, 5)
pres_train <- bradypus[group != 1, ]
pres_test <- bradypus[group == 1, ]
ext <- extent(-90, -32, -33, 23)
set.seed(10)
backg <- randomPoints(pred_nf, n=1000, ext=ext, extf = 1.25)
colnames(backg) = c('lon', 'lat')
group <- kfold(backg, 5)
backg_train <- backg[group != 1, ]
backg_test <- backg[group == 1, ]
train <- rbind(pres_train, backg_train)
pb_train <- c(rep(1, nrow(pres_train)), rep(0, nrow(backg_train)))
envtrain <- extract(predictors, train)
envtrain <- data.frame( cbind(pa=pb_train, envtrain) )
envtrain[,'biome'] = factor(envtrain[,'biome'], levels=1:14)

testpres <- data.frame( extract(predictors, pres_test) )
testbackg <- data.frame( extract(predictors, backg_test) )
testpres[ ,'biome'] = factor(testpres[ ,'biome'], levels=1:14)
testbackg[ ,'biome'] = factor(testbackg[ ,'biome'], levels=1:14)
```

```{r}
#first example
gm1 <- glm(pa ~ bio1 + bio5 + bio6 + bio7 + bio8 + bio12 + bio16 + bio17,
            family = binomial(link = "logit"), data=envtrain)
summary(gm1)

coef(gm1)

#second example
gm2 <- glm(pa ~ bio1+bio5 + bio6 + bio7 + bio8 + bio12 + bio16 + bio17,
            family = gaussian(link = "identity"), data=envtrain)
evaluate(testpres, testbackg, gm1)

ge2 <- evaluate(testpres, testbackg, gm2)
ge2

#show as a plot graph
pg <- predict(predictors, gm2, ext=ext)
par(mfrow=c(1,2))
plot(pg, main='GLM/gaussian, raw values')
plot(wrld_simpl, add=TRUE, border='dark grey')
tr <- threshold(ge2, 'spec_sens')
plot(pg > tr, main='presence/absence')
plot(wrld_simpl, add=TRUE, border='dark grey')
points(pres_train, pch='+')
points(backg_train, pch='-', cex=0.25)
```


# Step 6: Model evaluation

Most model types have different measures that can help to assess how good the model
fits the data. Most statistics or machine learning texts will provide
some details. For instance, for a GLM one can look at how much deviance is explained, whether there are patterns in the residuals, whether there are points with high leverage and so on. However, since many models are to be used for prediction, much evaluation is focused on how well the model predicts to points not used in model training.
What do we consider to evaluate a model?

Does the model seem sensible, ecologically?
Do the fitted functions (the shapes of the modeled relationships) make sense?
Do the predictions seem reasonable? (map them, and think about them)?
Are there any spatial patterns in model residuals? 


Different measures can be used to evaluate the quality of a prediction, perhaps depending on the goal of the study. Many measures for evaluating models based on presence-absence or presence-only data are ‘threshold dependent’. That means that a threshold must be set first (e.g., 0.5, though 0.5 is rarely a sensible choice – e.g. see Lui et al. 2005). Predicted values above that threshold indicate a prediction of ‘presence’, and values below the threshold indicate ‘absence’. Some measures emphasize the weight of false absences; others give more weight to false presences.

Much used statistics that are threshold independent are the correlation coefficient and the Area Under the Receiver Operator Curve (AUROC, generally further abbreviated to AUC). AUC is a measure of rank-correlation. In unbiased data, a high AUC indicates that sites with high predicted suitability values tend to be areas of known presence and locations with lower model prediction values tend to be areas where the species is not known to be present (absent or a random point). An AUC score of 0.5 means that the model is as good as a random guess. 

For a discussion on the use of AUC in the context of presence-only rather than presence/absence data.

Here we illustrate the computation of the correlation coefficient and AUC with two random variables. p (presence) has higher values, and represents the predicted value for 50 known cases (locations) where the species is present, and a (absence) has lower values, and represents the predicted value for 50 known cases (locations) where the species is absent.
```{r}
p <- rnorm(50, mean=0.7, sd=0.3)
a <- rnorm(50, mean=0.4, sd=0.4)
par(mfrow=c(1, 2))
plot(sort(p), col='red', pch=21)
points(sort(a), col='blue', pch=24)
legend(1, 0.95 * max(a,p), c('presence', 'absence'),
          pch=c(21,24), col=c('red', 'blue'))
comb <- c(p,a)
group <- c(rep('presence', length(p)), rep('absence', length(a)))
boxplot(comb~group, col=c('blue', 'red'))
```

We created two variables with random normally distributed values, but with different mean and standard deviation.
The two variables clearly have different distributions, and the values for ‘presence’ tend to be higher than for ‘absence’. 
Here is how you can compute the correlation coefficient and the AUC:
```{r}
group = c(rep(1, length(p)), rep(0, length(a)))
cor.test(comb, group)$estimate
```


```{r}
mv <- wilcox.test(p,a)
auc <- as.numeric(mv$statistic) / (length(p) * length(a))
auc
```

Below we show how you can compute these, and other statistics more conveniently, with the evaluate function in the dismo package. See ?evaluate for info on additional evaluation measures that are available. ROC/AUC can also be computed with the ROCR package.
```{r}
e <- evaluate(p=p, a=a)
class(e)

par(mfrow=c(1, 2))
density(e)
boxplot(e, col=c('blue', 'red'))
```

Back to some real data, presence-only in this case. We’ll divide the data in two random sets, one for training a Bioclim
model, and one for evaluating the model.

```{r}
samp <- sample(nrow(sdmdata), round(0.75 * nrow(sdmdata)))
traindata <- sdmdata[samp,]
traindata <- traindata[traindata[,1] == 1, 2:9]
testdata <- sdmdata[-samp,]
bc <- bioclim(traindata)
e <- evaluate(testdata[testdata==1,], testdata[testdata==0,], bc)
e
```


```{r}
plot(e, 'ROC')
```

Let’s first create presence and background data.

```{r}
pres <- sdmdata[sdmdata[,1] == 1, 2:9]
back <- sdmdata[sdmdata[,1] == 0, 2:9]
```

The background data will only be used for model testing and does not need to be partitioned. We now partition the data into 5 groups

```{r}
k <- 5
group <- kfold(pres, k)
group[1:10]

unique(group)

```
Now we can fit and test our model five times. In each run, the records corresponding to one of the five groups is only used to evaluate the model, while the other four groups are only used to fit the model. The results are stored in a list called ‘e’.

```{r}
e <- list()
for (i in 1:k) {
    train <- pres[group != i,]
    test <- pres[group == i,]
    bc <- bioclim(train)
    e[[i]] <- evaluate(p=test, a=back, bc)
}
```

We can extract several things from the objects in ‘e’, but let’s restrict ourselves to the AUC values and the “maximum of the sum of the sensitivity (true positive rate) and specificity (true negative rate)” threshold “spec_sens” (this is sometimes uses as a threshold for setting cells to presence or absence).

```{r}
auc <- sapply(e, function(x){x@auc})
auc
```


```{r}
mean(auc)

sapply( e, function(x){ threshold(x)['spec_sens'] } )
```
The use of AUC in evaluating SDMs has been criticized (Lobo et al. 2008, Jiménez-Valverde 2011). A particularly sticky problem is that the values of AUC vary with the spatial extent used to select background points. Generally, the larger that extent, the higher the AUC value. Therefore, AUC values are generally biased and cannot be directly compared. Hijmans (2012) suggests that one could remove “spatial sorting bias” (the difference between the distance from testing-presence to training-presence and the distance from testing-absence to training-presence points) through “point-wise distance sampling”.


```{r}
file <- file.path(system.file(package="dismo"), "ex/bradypus.csv")
bradypus <- read.table(file,  header=TRUE,  sep=',')
bradypus <- bradypus[,-1]
presvals <- extract(predictors, bradypus)
set.seed(0)
backgr <- randomPoints(predictors, 500)
nr <- nrow(bradypus)
s <- sample(nr, 0.25 * nr)
pres_train <- bradypus[-s, ]
pres_test <- bradypus[s, ]
nr <- nrow(backgr)
set.seed(9)
s <- sample(nr, 0.25 * nr)
back_train <- backgr[-s, ]
back_test <- backgr[s, ]
```
 
 
 
```{r}
sb <- ssb(pres_test, back_test, pres_train)
sb[,1] / sb[,2]
```

sb[,1] / sb[,2] is an indicator of spatial sorting bias (SSB). If there is no SSB this value should be 1, in these
data it is close to zero, indicating that SSB is very strong. Let’s create a subsample in which SSB is removed.

```{r}
i <- pwdSample(pres_test, back_test, pres_train, n=1, tr=0.1)
pres_test_pwd <- pres_test[!is.na(i[,1]), ]
back_test_pwd <- back_test[na.omit(as.vector(i)), ]
sb2 <- ssb(pres_test_pwd, back_test_pwd, pres_train)
sb2[1]/ sb2[2]
```

Spatial sorting bias is much reduced now; notice how the AUC dropped!

```{r}
bc <- bioclim(predictors, pres_train)
evaluate(bc, p=pres_test, a=back_test, x=predictors)
```



```{r}
evaluate(bc, p=pres_test_pwd, a=back_test_pwd, x=predictors)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

